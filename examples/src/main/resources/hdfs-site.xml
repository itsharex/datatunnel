<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
       Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <!--  指定命名空间  -->
    <property>
        <name>dfs.nameservices</name>
        <value>newns</value>
    </property>
<!--  指定该命名空间下的两个机器作为我们的NameNode  -->
    <property>
        <name>dfs.ha.namenodes.newns</name>
        <value>nn1,nn2</value>
    </property>

    <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>                   
        <value>false</value>
    </property>

    <!-- 配置第一台服务器的namenode通信地址  -->
    <property>
        <name>dfs.namenode.rpc-address.newns.nn1</name>
        <value>node1.hadoop01.aloudata.work:8020</value>
    </property>
    <!--  配置第二台服务器的namenode通信地址  -->
    <property>
        <name>dfs.namenode.rpc-address.newns.nn2</name>
        <value>node2.hadoop01.aloudata.work:8020</value>
    </property>
    <!-- 所有从节点之间相互通信端口地址 -->
    <property>
        <name>dfs.namenode.servicerpc-address.newns.nn1</name>
        <value>node1.hadoop01.aloudata.work:8022</value>
    </property>
    <!-- 所有从节点之间相互通信端口地址 -->
    <property>
        <name>dfs.namenode.servicerpc-address.newns.nn2</name>
        <value>node2.hadoop01.aloudata.work:8022</value>
    </property>

    <!-- 第一台服务器namenode的web访问地址  -->
    <property>
        <name>dfs.namenode.http-address.newns.nn1</name>
        <value>node1.hadoop01.aloudata.work:50070</value>
    </property>
    <!-- 第二台服务器namenode的web访问地址  -->
    <property>
        <name>dfs.namenode.http-address.newns.nn2</name>
        <value>node2.hadoop01.aloudata.work:50070</value>
    </property>

    <!-- journalNode的访问地址，注意这个地址一定要配置 -->
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://node1.hadoop01.aloudata.work:8485;node2.hadoop01.aloudata.work:8485;node3.hadoop01.aloudata.work:8485/newns</value>
    </property>
    <!--  指定故障自动恢复使用的哪个java类 -->
    <property>
        <name>dfs.client.failover.proxy.provider.newns</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
<!-- 故障转移使用的哪种通信机制 -->        <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>

    <!-- 指定通信使用的公钥  -->
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/data/.id_rsa</value>
    </property>
    <!-- journalNode数据存放地址  -->
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/data/data/hadoop/dfs/jn</value>
    </property>
    <!-- 启用自动故障恢复功能 -->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <!-- namenode产生的文件存放路径 -->
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///data/data/hadoop/dfs/nn/name</value>
    </property>
    <!-- edits产生的文件存放路径 -->
    <property>
        <name>dfs.namenode.edits.dir</name>
        <value>file:///data/data/hadoop/dfs/nn/edits</value>
    </property>
    <!-- dataNode文件存放路径 -->
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///data/data/hadoop/dfs/dn</value>
    </property>
    <!-- 关闭hdfs的文件权限 -->
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <!-- 指定block文件块的大小 -->
    <property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
    </property>
    <property>
        <name>dfs.namenode.acls.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.client.use.legacy.blockreader</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.client.read.shortcircuit</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.domain.socket.path</name>
        <value>/var/run/hdfs-sockets/dn</value>
    </property>
    <property>
        <name>dfs.client.read.shortcircuit.skip.checksum</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.client.domain.socket.data.traffic</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.block.access.token.enable</name>
        <value>true</value>
    </property>
    <property>
        <name>ipc.client.fallback-to-simple-auth-allowed</name>
        <value>true</value> 
    </property>

    <!--
    ecs s3: http://doc.isilon.com/ECS/3.3/DataAccessGuide_old/vipr_r_hdfs_core_site_properties_ecs_s3.html
    -->
    <property>
        <name>fs.s3a.impl</name>
        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
    </property>
    <property>
        <name>fs.s3a.connection.ssl.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>fs.s3a.path.style.access</name>
        <value>true</value>
    </property>
</configuration>
